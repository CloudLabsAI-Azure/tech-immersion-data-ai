{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required libraries\n",
    "\n",
    "After your install these libraries it is recommended that you **restart** the notebook kernel from the **Kernel** menu above. After restarting the kernel, start from the **Train a deep learning model** section.\n",
    "\n",
    "You can ignore any incompatibility errors. Install the libraries **only once**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore errors related to tensorflow-gpu compatibility. **Installing tensorflow can take more than 5 minutes. Please be patient if it appears to be hung or stuck for few minutes during installation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras==2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore errors related to compatibility. As long as you see the message at the end: **Successfully installed onnxruntime-1.3.0** OR **Requirement already satisfied**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxmltools==1.6.1\n",
    "!pip install keras2onnx==1.6.1\n",
    "!pip install onnxruntime==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a deep learning model\n",
    "In this notebook you will train a deep learning model to classify the descriptions of car components as compliant or non-compliant. \n",
    "\n",
    "Each document in the supplied training data set is a short text description of the component as documented by an authorized technician. \n",
    "The contents include:\n",
    "- Manufacture year of the component (e.g. 1985, 2010)\n",
    "- Condition of the component (poor, fair, good, new)\n",
    "- Materials used in the component (plastic, carbon fiber, steel, iron)\n",
    "\n",
    "The compliance regulations dictate:\n",
    "*Any component manufactured before 1995 or in fair or poor condition or made with plastic or iron is out of compliance.*\n",
    "\n",
    "For example:\n",
    "* Manufactured in 1985 made of steel in fair condition -> **Non-compliant**\n",
    "* Good condition carbon fiber component manufactured in 2010 -> **Compliant**\n",
    "* Steel component manufactured in 1995 in fair condition -> **Non-Compliant**\n",
    "\n",
    "The labels present in this data are 0 for compliant, 1 for non-compliant.\n",
    "\n",
    "The challenge with classifying text data is that deep learning models only undertand vectors (e.g., arrays of numbers) and not text. To encode the car component descriptions as vectors, we use an algorithm from Stanford called [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/). GloVe provides us pre-trained vectors that we can use to convert a string of text into a vector. \n",
    "\n",
    "## Setup\n",
    "To begin, you will need to provide the following information about your Azure Subscription. \n",
    "\n",
    "In the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*). Also provide the values for the pre-created GPU type AML compute (`cluster_name`). \n",
    "\n",
    "If you are in doubt of any of these values, do the following:\n",
    "1. Navigate to the Azure Portal and login with the credentials provided.\n",
    "2. From the left hand menu, under Favorites, select `Resource Groups`.\n",
    "3. In the list, select the resource group with the name similar to `tech_immersion_XXXXX`.\n",
    "4. From the Overview tab, capture the desired values.\n",
    "\n",
    "Execute the following cell by selecting the `>|Run` button in the command bar above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id = \"\" # <- needs to be the subscription with the resource group\n",
    "\n",
    "#Provide values for the existing Resource Group \n",
    "resource_group = \"tech_immersion_xxxxx\" # <- replace xxxxx with the values from your ONNX resource group name\n",
    "\n",
    "#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
    "workspace_name = \"tech_immersion_aml_xxxxx\" # <- replace xxxxx with the values from your resource group name\n",
    "workspace_region = \"eastus\" # <- region of your resource group \n",
    "#(other options include eastus, westcentralus, southeastasia, australiaeast, westeurope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can leave these values as they are or experiment with changing them after you have completed the notebook once\n",
    "experiment_name = 'deep-learning'\n",
    "project_folder = './dl'\n",
    "deployment_folder = './deploy'\n",
    "onnx_export_folder = './onnx'\n",
    "\n",
    "# this is the URL to the CSV file containing the GloVe vectors\n",
    "glove_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "             'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "             'quickstarts/connected-car-data/glove.6B.100d.txt')\n",
    "\n",
    "# this is the URL to the CSV file containing the care component descriptions\n",
    "data_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "            'quickstarts/connected-car-data/connected-car_components.csv')\n",
    "\n",
    "# this is the name of the AML GPU Compute cluster\n",
    "cluster_name = \"aml-compute-gpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Azure Machine Learning resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Azure Machine Learning SDK provides a comprehensive set of a capabilities that you can use directly within a notebook including:\n",
    "- Creating a **Workspace** that acts as the root object to organize all artifacts and resources used by Azure Machine Learning.\n",
    "- Creating **Experiments** in your Workspace that capture versions of the trained model along with any desired model performance telemetry. Each time you train a model and evaluate its results, you can capture that run (model and telemetry) within an Experiment.\n",
    "- Creating **Compute** resources that can be used to scale out model training, so that while your notebook may be running in a lightweight container in Azure Notebooks, your model training can actually occur on a powerful cluster that can provide large amounts of memory, CPU or GPU. \n",
    "- Using **Automated Machine Learning (AutoML)** to automatically train multiple versions of a model using a mix of different ways to prepare the data and different algorithms and hyperparameters (algorithm settings) in search of the model that performs best according to a performance metric that you specify. \n",
    "- Packaging a Docker **Image** that contains everything your trained model needs for scoring (prediction) in order to run as a web service.\n",
    "- Deploying your Image to either Azure Kubernetes or Azure Container Instances, effectively hosting the **Web Service**.\n",
    "\n",
    "In Azure Notebooks or Notebook VMs, all of the libraries needed for Azure Machine Learning are pre-installed. To use them, you just need to import them. Run the following cell to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import Image\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core import Environment\n",
    "from azureml.core.environment import CondaDependencies\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and connect to an Azure Machine Learning Workspace\n",
    "Run the following cell to create a new Azure Machine Learning **Workspace** and save the configuration to disk (next to the Jupyter notebook). \n",
    "\n",
    "**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using the exist_ok param, if the worskpace already exists you get a reference to the existing workspace\n",
    "# allowing you to re-run this cell multiple times as desired (which is fairly common in notebooks).\n",
    "ws = Workspace.create(\n",
    "    name = workspace_name,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group = resource_group, \n",
    "    location = workspace_region,\n",
    "    exist_ok = True)\n",
    "\n",
    "ws.write_config()\n",
    "print('Workspace configuration succeeded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Compute Cluster\n",
    "Now you are ready to create the GPU compute cluster. Run the following cell to create a new compute cluster (or retrieve the existing cluster if it already exists). The code below will create a *GPU based* cluster where each node in the cluster is of the size `Standard_NC12`, and the cluster will start with at 1 such node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create AML GPU based Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6',\n",
    "                                                           min_nodes=1, max_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a ScriptRunConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `ScriptRunConfig` allows you to define the job that will execute when you run an experiment. The `AzureML-TensorFlow-2.2-GPU` curated enviroment is specially configured to handle executing runs that use TensorFlow, such as the Keras training script you will create shortly. Run the following cell to create the `ScriptRunConfig` with enviroment `AzureML-TensorFlow-2.2-GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env_name = \"dnn-env\"\n",
    "myenv = Environment.get(workspace=ws, name=\"AzureML-TensorFlow-2.2-GPU\").clone(my_env_name)\n",
    "conda_dep = myenv.python.conda_dependencies\n",
    "conda_dep.add_pip_package(\"numpy==1.18.5\")\n",
    "conda_dep.add_pip_package(\"pandas==1.0.4\")\n",
    "conda_dep.add_pip_package(\"keras==2.3.1\")\n",
    "conda_dep.add_pip_package(\"onnxmltools==1.6.1\")\n",
    "conda_dep.add_pip_package(\"keras2onnx==1.6.1\")\n",
    "conda_dep.add_pip_package(\"onnxruntime==1.3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ScriptRunConfig(source_directory=project_folder,\n",
    "                         script='train.py',\n",
    "                         compute_target=compute_target,\n",
    "                         environment=myenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remotely train a deep learning model using the Azure ML Compute\n",
    "In the following cells, you will *not* train the model against the data you just downloaded using the resources provided by Notebook VM. Instead, you will deploy an Azure ML Compute cluster that will download the data and use a trainings script to train the model. In other words, all of the training will be performed remotely with respect to this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create project folder\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/train.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras import models \n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "def get_data():\n",
    "    data_url = \"https://databricksdemostore.blob.core.windows.net/data/connected-car/connected-car_components.csv\"\n",
    "    car_components_df = pd.read_csv(data_url)\n",
    "    components = car_components_df[\"text\"].tolist()\n",
    "    labels = car_components_df[\"label\"].tolist()\n",
    "    return { \"components\" : components, \"labels\" : labels }\n",
    "\n",
    "def download_glove():\n",
    "    print(\"Downloading GloVe embeddings...\")\n",
    "    import urllib.request\n",
    "    glove_url = 'https://databricksdemostore.blob.core.windows.net/data/connected-car/glove.6B.100d.txt'\n",
    "    urllib.request.urlretrieve(glove_url, 'glove.6B.100d.txt')\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "download_glove()\n",
    "\n",
    "# Load the car components labeled data\n",
    "print(\"Loading car components data...\")\n",
    "data_url = \"https://databricksdemostore.blob.core.windows.net/data/connected-car/connected-car_components.csv\"\n",
    "car_components_df = pd.read_csv(data_url)\n",
    "components = car_components_df[\"text\"].tolist()\n",
    "labels = car_components_df[\"label\"].tolist()\n",
    "print(\"Loading car components data completed.\")\n",
    "\n",
    "# split data 60% for trianing, 20% for validation, 20% for test\n",
    "print(\"Splitting data...\")\n",
    "train, validate, test = np.split(car_components_df.sample(frac=1), [int(.6*len(car_components_df)), \n",
    "                                                                    int(.8*len(car_components_df))])\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)\n",
    "\n",
    "# use the Tokenizer from Keras to \"learn\" a vocabulary from the entire car components text\n",
    "print(\"Tokenizing data...\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100                                           \n",
    "training_samples = 90000                                 \n",
    "validation_samples = 5000    \n",
    "max_words = 10000      \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(components)\n",
    "sequences = tokenizer.texts_to_sequences(components)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])                     \n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "x_test = data[training_samples + validation_samples:]\n",
    "y_test = labels[training_samples + validation_samples:]\n",
    "print(\"Tokenizing data complete.\")\n",
    "\n",
    "# apply the vectors provided by GloVe to create a word embedding matrix\n",
    "print(\"Applying GloVe vectors...\")\n",
    "glove_dir =  './'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector    \n",
    "print(\"Applying GloVe vectors compelted.\")\n",
    "\n",
    "# use Keras to define the structure of the deep neural network   \n",
    "print(\"Creating model structure...\")\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# fix the weights for the first layer to those provided by the embedding matrix\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "print(\"Creating model structure completed.\")\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "print(\"Training model completed.\")\n",
    "\n",
    "print(\"Saving model files...\")\n",
    "# create a ./outputs/model folder in the compute target\n",
    "# files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
    "os.makedirs('./outputs/model', exist_ok=True)\n",
    "model.save('./outputs/model/model.h5')\n",
    "print(\"model saved in ./outputs/model folder\")\n",
    "print(\"Saving model files completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code pattern to submit a training run to Azure Machine Learning compute targets is always:\n",
    "\n",
    "- Create an experiment to run.\n",
    "- Submit the experiment.\n",
    "- Wait for the run to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the run to complete by executing the following cell. Note that this process will perform the following:\n",
    "- Build and deploy the container to Azure Machine Learning compute (~25-30 minutes)\n",
    "- Execute the training script (~3 minutes)\n",
    "\n",
    "If you change only the training script and re-submit, it will run faster the second time because the necessary container is already prepared so the time requried is just that for executing the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the training script\n",
    "The training script shown previously does a lot, lets break it apart into smaller parts that you can run locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the GloVe embeddings to your environment.\n",
    "Run the following cell to download the embeddings to the `data` folder in your environment. Note: this may take a **few minutes** as the GloVe file is about 340 MB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "urllib.request.urlretrieve(glove_url, './data/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to import the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras import models \n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have downloaded the data, load it into a Pandas DataFrame by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the car components labeled data\n",
    "car_components_df = pd.read_csv(data_url)\n",
    "components = car_components_df[\"text\"].tolist()\n",
    "labels = car_components_df[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data 60% for training, 20% for validation, 20% for test\n",
    "train, validate, test = np.split(car_components_df.sample(frac=1), [int(.6*len(car_components_df)), int(.8*len(car_components_df))])\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, you use the Tokenizer from Keras to \"learn\" a vocabulary from the entire car components text. Then the data (both the text and the compliance labels) is split into three subsets, one that will be used for training the deep learning model, one that will be used during training batches to tune the model weights and one that will be used after the model is trained to evaluate how it performs on data the model has never seen. \n",
    "\n",
    "Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100                                           \n",
    "training_samples = 90000                                 \n",
    "validation_samples = 5000    \n",
    "max_words = 10000      \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(components)\n",
    "sequences = tokenizer.texts_to_sequences(components)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])                     \n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "x_test = data[training_samples + validation_samples:]\n",
    "y_test = labels[training_samples + validation_samples:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at how the text was encoded as an array in the above. Run the following cell to take a peek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The text '{text}' is represented as the vector '{data}'\".format(text=components[indices[0]], data=x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will apply the vectors provided by GloVe to create a word embedding matrix. This matrix will be used shortly to set the model wights of the first layer of the deep neural network. \n",
    "\n",
    "Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir =  './data'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, you will use Keras to define the structure of the deep neural network. The network graph you build in this case has four layers. \n",
    "\n",
    "Run the following cell to structure the network and view a summary description of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than train model from scratch on this car components text, you can instead use the embedding matrix derived from GloVe. In effect this boosts the model's understanding of text, because the GloVe vectors was trained against a large corpus of text in Wikipedia. \n",
    "\n",
    "Run the following cell to fix the weights for the first layer to those provided by the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to train the model.\n",
    "\n",
    "Run the following cell to train the model. This will take **3-4 minutes** if you are using CPU instead of GPU enabled cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=3,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.save_weights(os.path.join(glove_dir,'pre_trained_glove_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look to see how the model training went. If curves for training accuracy and validation accuracy come together, you are in good shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also evaluate how accurately the model performs against data it has not seen. Run the following cell to see the accuracy (it is the second number in the array displayed, on a scale from 0 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(glove_dir,'pre_trained_glove_model.h5'))\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have explored how the model is trained and used locally. Let's return to the model you had previously trained remotely using the Azure Machine Learning compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model files from the run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training script, the Keras model is saved into two files, model.json and model.h5, in the outputs/models folder on the GPU cluster AmlCompute node. Azure ML automatically uploaded anything written in the ./outputs folder into run history file store. Subsequently, we can use the run object to download the model files. They are under the the outputs/model folder in the run history file store, and are downloaded into a local folder named model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model folder in the current directory\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs/model'):\n",
    "        output_file_path = os.path.join('./model', f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "output_folder = './model'\n",
    "model_filename = 'model.h5'\n",
    "\n",
    "model = load_model(os.path.join(output_folder, model_filename))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a Keras model to ONNX\n",
    "In the steps that follow, you will convert Keras model you just trained to the ONNX format. This will enable you to use this model for classification in a very broad range of environments including:\n",
    "\n",
    "- Web services \n",
    "- iOS and Android mobile apps\n",
    "- Windows apps\n",
    "- IoT devices\n",
    "\n",
    "Converting a Keras model requires the use of the `onnxmltools`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the model to ONNX by running the following cell.\n",
    "\n",
    "*Ignore the WARNING - Can't import tf2onnx module, so the conversion on a model with any custom/lambda layer will fail!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployment folder in the current directory\n",
    "os.makedirs(deployment_folder, exist_ok=True)\n",
    "\n",
    "# create an onnx subfolder under deployment\n",
    "os.makedirs(os.path.join(deployment_folder, onnx_export_folder), exist_ok=True)\n",
    "\n",
    "import onnxmltools\n",
    "\n",
    "# Convert the Keras model to ONNX\n",
    "onnx_model_name = 'component_compliance.onnx'\n",
    "converted_model = onnxmltools.convert_keras(model, onnx_model_name, target_opset=7)\n",
    "\n",
    "# Save the model locally...\n",
    "onnx_model_path = os.path.join(deployment_folder, onnx_export_folder)\n",
    "os.makedirs(onnx_model_path, exist_ok=True)\n",
    "onnxmltools.utils.save_model(converted_model, os.path.join(onnx_model_path,onnx_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell created a new file called `component_compliance.onnx` that contains the ONNX version of the model. \n",
    "\n",
    "Now try using this ONNX model to classify a component description by running the following cell. Remeber the prediction will be a value close to 0 (non-compliant) or to 1 (compliant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_session = onnxruntime.InferenceSession(os.path.join(os.path.join(deployment_folder, \n",
    "                                                                      onnx_export_folder), onnx_model_name))\n",
    "\n",
    "# Grab one sample from the test data set\n",
    "test_sample = np.reshape(x_test.astype(np.float32)[0], (1,100))\n",
    "\n",
    "# Run an ONNX session to classify the sample.\n",
    "classify_output = onnx_session.run(None, {onnx_session.get_inputs()[0].name:test_sample})[0] \n",
    "\n",
    "print(\"Your model predicted the class `{pred}`, and the actual class was `{actual}`\".format(\n",
    "    pred=classify_output[0][0], actual=y_test[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Deep Learning ONNX format model as a web service\n",
    "To demonstrate one example of using the ONNX format model in a new environment, you will deploy the ONNX model to a webservice. On the web server, the only component required by the model is the ONNX Runtime, which is used to load the model and use it for scoring. Neither Keras nor TensorFlow are required on the web server.\n",
    "\n",
    "In this case, you will use the Azure Machine Learning service SDK to programmatically create a Workspace, register your model, create a container image for the web service that uses it and deploy that image on to an Azure Container Instance.\n",
    "\n",
    "Run the following cells to create some helper functions that you will use for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Model with AML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Registering and uploading model...\")\n",
    "model_folder_path=os.path.join(deployment_folder, onnx_export_folder)\n",
    "model_name=\"component_compliance\"\n",
    "\n",
    "registered_model = Model.register(model_path=model_folder_path, \n",
    "                                  model_name=model_name, \n",
    "                                  workspace=ws)\n",
    "\n",
    "print(registered_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Scoring Script\n",
    "\n",
    "Your web service which knows how to load the model and use it for scoring needs saved out to a file for the Azure Machine Learning service SDK to deploy it. Run the following cell to create this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scoring_service.py\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azureml.core.model import Model\n",
    "import onnxruntime\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    \n",
    "    try:\n",
    "        model_path = Model.get_model_path('component_compliance')\n",
    "        model_file_path = os.path.join(model_path,'component_compliance.onnx')\n",
    "        print('Loading model from:', model_file_path)\n",
    "        \n",
    "        # Load the ONNX model\n",
    "        model = onnxruntime.InferenceSession(model_file_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        print(\"Received input:\", raw_data)\n",
    "        \n",
    "        input_data = np.array(json.loads(raw_data)).astype(np.float32)\n",
    "        \n",
    "        # Run an ONNX session to classify the input.\n",
    "        result = model.run(None, {model.get_inputs()[0].name:input_data})[0] \n",
    "        result = result[0][0].item()\n",
    "        \n",
    "        # return just the classification index (0 or 1)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create InferenceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = InferenceConfig(entry_script=\"scoring_service.py\", source_directory='./', environment=myenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model to Azure Container Instance (ACI) as a Web Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'Compliance classification web service.'\n",
    "\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "                        cpu_cores=3, \n",
    "                        memory_gb=15, \n",
    "                        location='eastus', \n",
    "                        description=description, \n",
    "                        auth_enabled=True, \n",
    "                        tags = {'name': 'ACI container', \n",
    "                                'model_name': registered_model.name, \n",
    "                                'model_version': registered_model.version\n",
    "                                }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note it can take **5-10 minutes** for the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service_name=\"complianceservice\"\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=aci_service_name,\n",
    "                       models=[registered_model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config= aci_config, \n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Deployment\n",
    "\n",
    "Finally, test your deployed web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# choose a sample from the test data set to send\n",
    "test_sample = np.reshape(x_test.astype(np.float32)[0], (1,100))\n",
    "test_sample_json = json.dumps(test_sample.tolist())\n",
    "\n",
    "# invoke the web service\n",
    "result = service.run(input_data=test_sample_json)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a working web service deployed that uses the ONNX version of your Keras deep learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "Deep Learning",
  "notebookId": 2340934485665719
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
